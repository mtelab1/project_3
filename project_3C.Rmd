---
title: "DATA 607 - Project 3"
author: "Team Silver Fox"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    code_folding: hide
    df_print: paged
    highlight: tango
    #number_sections: yes
    smooth_scroll: yes
    theme: united
    toc_collapsed: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    extra_dependencies:
      -geometry
      -multicol
      -multirow
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '5'
    toc_float: yes
  word_document:
    toc: year
    toc_depth: "5"
theme: lumen
number_sections: yes
toc_depth: 3
---


```{r load-packages, include=FALSE}
library(tidyverse)
library(openintro)
library(dplyr)
library(tidyr)
library(RCurl)
library(ggplot2)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### BACKGROUND

> What are the most valued data science skills?
Answering this question formed the crux of our project. 

We collaborated in understanding the question at hand, brainstorming an approach and where we might pull data from, deciding on a dataset and then acquiring, tidying & transforming, visualizing & analyzing this dataset before ultimately coming to the conclusion we'll present later.

*Team Silver Fox*  collaborated virtually on Slack, Google Docs, Google Meet, and Github. There were numerous adaptations to our approach along the way and we ultimately decided on a "twice sifted" approach to answering the question above.

Without further adieu, let’s introduce *Team Silver Fox*:

* **Dan Rosenfeld** – [Github link]
* **Magnus Skonberg** – [Github](https://github.com/Magnus-PS/)
* **Mustafa Telab** – [Github](https://github.com/mtelab1)
* **Josef Waples** - [Github](https://github.com/hachiko1024)

--------------------------------------------------------------------------------

#### DATA SOURCE(S)

We identified our sources of data as the following (cited APA-style below):

1. Kaggle. (2018). **2018 Kaggle ML & DS Survey** [Data file]. Retrieved from https://www.kaggle.com/kaggle/kaggle-survey-2018?select=multipleChoiceResponses.csv

2. Jeff Hale. (2018). **The Most in Demand Skills for Data Scientists** [Data file]. Retrieved from https://www.kaggle.com/discdiver/the-most-in-demand-skills-for-data-scientists/log?select=ds_job_listing_software.csv

For more background on Data Source #1:

* The **2018 Kaggle ML & DS Survey** dataset had (3) .csv’s: the Survey Schema, Free Form Responses, and Multiple Choice Responses. 

* *The Survey Schema* contained questions and explanations of response exemptions. While we do not plan on analyzing this .csv, it was useful pre-screening questions for pertinence. 

* *The Free Form Responses* contained text answers submitted by survey takers when their response was not covered by multiple choice options. If we deem this set valuable / unique for purposes of analysis, we will draw on it. Otherwise, it may be disregarded as well.

* *The Multiple Choice Responses* contain categorical variables submitted by respondents. This is where our “gold” lies and likely where the brunt of our data tidying, transforming, and analysis efforts will go.

--------------------------------------------------------------------------------

#### APPROACH

We started our search by utilizing Kaggle’s self proclaimed “most comprehensive dataset available on the state of ML and data science”, pulling what we deemed useful from this set.

From our findings of the general dataset, we then applied a “value filter” (ie. only considering responses above a certain income, education, or experience level) to see how the general perspective of essential skills differed from that of a more seasoned (or market-valued) perspective.

At this point, we then transitioned to comparing data science skills deemed as valuable in Kaggle's 2018 survey with the most in-demand skills for data scientists across multiple job sites. This "pro level" dataset (also from 2018) was prepared by Jeff Hale, and drawn on to paint the picture of contrast (if there was any) between the skills data scientists deem as most valuable (ie. those doing the work) vs. those that employers hire for (ie. hiring managers).

By contrasting our sets via different techniques and visualizations, our findings were twice sifted. Hence the mention of a "twice sifted" approach in the *Background* section.

Our general approach is outlined below:

1. **Pre-screen:** after viewing the initial Kaggle **2018 Kaggle ML & DS Survey** multiple choice response dataset and seeing that we would not be able to access this dataset via Github due to its size (39+ MB), we deemed “pre-screening” essential. We reviewed all 50 questions from the SurveySchema.csv file and kept only those applicable to the question at hand. Once this dataset had been “pre-screened” it was uploaded to Github along with Jeff Hale’s general and software skill csv files (for later comparison).

2. **Acquire data via reading .csv's:** get the URL, read each .csv from Github (in its raw form), and put the data into tabular form before moving on to tidying and transforming.
3. **Tidy and transform:** by leaning on the dplyr() library and numerous useful R functions, we extracted a table of questions and a table of answers prior to extracting relevant keywords and counts (for later visualization). Our T&T process was heavy on the **2018 Kaggle ML & DS Survey** data and light for the **The Most in Demand Skills for Data Scientists** data. 
4. **Visualize and analyze**: once our data was in a more "digestible" form we set out to generate insightful plots regarding the most valuable **software skills** and **general skills**. We plotted (and interpreted) the survey data, and then applied a "value filter" to this data prior to introducing the plots for the **The Most in Demand Skills for Data Scientists** data. The purpose of this tiered approach was to sift for additional insight (if there was any to be found).
5. **Conclude**: after applying our tiered approach, we drew insight from the story our data told and came to a conclusion. More on this later :)

--------------------------------------------------------------------------------

### ACQUIRE DATA

First, we read our .csv's from Github (in their raw form) and then converted these datasets into tabular form. Our approach was to create and utilize a function to get corresponding URLs and read the csv files from these URLs. This was not the most line-efficient strategy but it was a unique way of extracting our csv's which some may deem a "rinse and repeat" process:

```{r}
#Store URLs
url1 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/pertinent_mc_resps.csv"
url2 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/ds_software.csv"
url3 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/ds_general.csv"
#Function to get URLs and read in corresponding .csv files
url_to_dataframe_function <- function(x, y, z) {
  url <- getURL(url1)
  data1 <- read.csv(text = url)
  
  url2 <- getURL(url2)
  data2 <- read.csv(text = url2)
  
  url3 <- getURL(url3)
  data3 <- read.csv(text = url3)
  
  return(list(data1, data2, data3))
  }
#Call to function (defined above)
dataframes <- url_to_dataframe_function(url1, url2, url3)
#Assignment of values
for (i in seq(dataframes))
  assign(paste0("data", i), dataframes[[i]])
#Conversion to dataframe and display for 2018 Kaggle ML & DS Survey multiple choice data and then The Most in Demand Skills for Data Scientists software and general skills
mc_resps <- as_tibble(data1)
tech_skills <- as_tibble(data2)
genl_skills <- as_tibble(data3)
```

--------------------------------------------------------------------------------

### TIDY & TRANSFORM

Once we had our data at hand, we set out to make sense of this data and put it in a more digestible (normalized) form for later visualization and analysis.

For this phase of the project, we added to the more ordinary of R functions with our use of the dplyr() library. We removed columns and rows, renamed columns, transposed our data into long form, gathered essential columns into different data frames, extracted relevant counts, and created "light" visualizations meant to provide insight as to what v. wasn't valuable and essential for the next phase. 

Our T&T process was heavy on the **2018 Kaggle ML & DS Survey** data and light for the **The Most in Demand Skills for Data Scientists** data, and took the following form: 


#### KAGGLE SURVEY QUESTIONS

We created a table of questions by extracting unique questions from the existing dataset, compressing repeated questions from multiple columns into one, renaming column titles for the same format (ie. Q4), transposing for a long dataframe, and replacing question numbers with question IDs.

Thus transforming a 2 x 43 subsection of the dataset into a 10 x 2 table:
```{r}
#Extract all unique questions into questions table
questions <- mc_resps[1,]
#Compress questions with multiple columns into one
comp_qs <- questions[ -c(6:12,14:31,33,35,38:43) ]
#Rename column titles for same format (ie. Q4)
names(comp_qs)[names(comp_qs) == "Q11_Part_1"] <- "Q11"
names(comp_qs)[names(comp_qs) == "Q16_Part_1"] <- "Q16"
names(comp_qs)[names(comp_qs) == "Q34_Part_1"] <- "Q34"
#Transpose for a long dataframe
qs_long <- gather(comp_qs, "Q #", "Question", 1:10, factor_key=TRUE)
#Replace the question number with a question ID (ie. Q4 --> 1)
qs_long$Q_ID <- seq.int(nrow(qs_long)) 
qs_long <- qs_long[ -c(1) ]
qs_final <- qs_long[c("Q_ID", "Question")]
```

#### KAGGLE SURVEY ANSWERS

We then created a table of answers and further widdled our dataset down by removing all columns with "OTHER" labels or non-valuable entries and removing 1st row question statements to create an answers table that would be drawn from for tables, plots, and analysis later on.

A small yet foundational step.

```{r}
#Further widdle our dataset down: remove all columns with "OTHER" labels and non-valuable entries, remove 1st row (question statement)
answers <- mc_resps[ -c(11:12,29:31,33,35,43) ]
answers <- answers[-1,]
```

Once we'd laid the foundation, we set out to draw counts from this table of answers for software skills, general skills, and the value filter ...

#### KAGGLE SURVEY SOFTWARE SKILLS

Below is the code for our creation of a table for unique variables related to SOFTWARE SKILLS from the **2018 Kaggle ML & DS Survey** data: 

>Could edit intro text for remaining sections to flow better ...<

```{r}
#Software language "regular basis" (Q16) count
##Gather associated answers into one column, cut out irrelevant columns, then count
reg <- gather(answers, key = "Q16_A_ID", value = "Q16_Answer", Q16_Part_1, Q16_Part_2, Q16_Part_3, Q16_Part_4, Q16_Part_5, Q16_Part_6, Q16_Part_7, Q16_Part_8, Q16_Part_9, Q16_Part_10, Q16_Part_11, Q16_Part_12, Q16_Part_13, Q16_Part_14, Q16_Part_15, Q16_Part_16)
reg <- reg[ -c(1:19) ]
reg <- reg %>% count(Q16_Answer, sort = TRUE)%>% rename(reg = n)
reg <- reg [-1,] #remove empty row

#Software language "use most often" (Q17) count
mo <- answers %>% count(Q17, sort = TRUE)%>% rename(mo = n)
mo <- mo[-1,] #remove empty row

#Software language "recommend to aspiring DSs" (Q18) count
rec <- answers %>% count(Q18, sort = TRUE)%>% rename(rec = n)
rec <- rec[-2,] #remove empty row

```

#### KAGGLE SURVEY GENERAL SKILLS

Below is the code for our creation of a table for unique variables related to GENERAL SKILLS from the **2018 Kaggle ML & DS Survey** data: 

```{r}
###Tidying: could pull keywords out for better plotting, analysis, etc. Phrases are too long to plot

#Machine learning (Q10) count - very specific. Programming languages are a gateway to this direction.
#ml_count <- answers %>% count(Q10, sort = TRUE)
#ml_count

#Daily activities (Q11) count - NOT USEFUL / UNIQUE
##Gather associated answers into one column, cut out irrelevant columns, then count
# all_daily <- gather(answers, key = "Q11_A_ID", value = "Q11_Answer", Q11_Part_1, Q11_Part_2, Q11_Part_3, Q11_Part_4, Q11_Part_5, Q11_Part_6)
# all_daily <- all_daily[ -c(1:29) ]
# all_daily %>% count(Q11_Answer, sort = TRUE)
#Software activity (Q23) count
sw <- answers %>% count(Q23, sort = TRUE)
sw <- sw[-2,] #remove empty row
sw$Q23 <- as.character(sw$Q23)
sw$Q23[sw$Q23 == "50% to 74% of my time"] <- "50%-74%"
sw$Q23[sw$Q23 == "25% to 49% of my time"] <- "25%-49%"
sw$Q23[sw$Q23 == "1% to 25% of my time"] <- "1%-25%"
sw$Q23[sw$Q23 == "75% to 99% of my time"] <- "75%-99%"
sw$Q23[sw$Q23 == "0% of my time"] <- "0%"
sw$Q23[sw$Q23 == "100% of my time"] <- "100%"

#Visualize software activity
ggplot(sw, aes(x = Q23, y = n)) +
  geom_col(fill = "#00abff") +
  labs(title = "Data Science Survey", subtitle = "% of school / workday actively coding", x = "", y = "n") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5)) +
  labs(caption = "2018") +
  scale_x_discrete(limits= c("0%", "1%-25%", "25%-49%","50%-74%", "75%-99%", "100%"))

#% use time (Q34) count - NOT USEFUL / UNIQUE
##Gather associated answers into one column, cut out irrelevant columns, then count
# all_time <- gather(answers, key = "Q34_A_ID", value = "Q34_Answer", Q34_Part_1, Q34_Part_2, Q34_Part_3, Q34_Part_4, Q34_Part_5, Q34_Part_6)
# all_time <- all_time[ -c(1:29) ]
# all_time %>% count(Q34_Answer, sort = TRUE)
```

#### KAGGLE SURVEY VALUE FILTER

Below is the code for our creation of a table for unique variables related to the VALUE FILTER from the **2018 Kaggle ML & DS Survey** data: 

```{r}
#Education level count
answers %>% count(Q4, sort = TRUE)
##Experience count
answers$Q8 <- as.character(answers$Q8)
answers$Q8[answers$Q8 == "10-May"] <- "5-10"
answers$Q8[answers$Q8 == "15-Oct"] <- "10-15"
answers$Q8[answers$Q8 == "4-Mar"] <- "3-4"
answers$Q8[answers$Q8 == "2-Jan"] <- "1-2"
answers$Q8[answers$Q8 == "3-Feb"] <- "2-3"
answers$Q8[answers$Q8 == "5-Apr"] <- "4-5"
#Storage of corresponding observation counts in "a"
exp_count <- answers %>% count(Q8, sort = TRUE)
#Visualization of years experience count
ggplot(exp_count, aes(x = Q8, y = n)) +
  geom_col(fill = "#00abff") +
  labs(title = "Data Science Survey", subtitle = "Years of Experience") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5)) +
  labs(caption = "2018", x = "", y = "") +
  scale_x_discrete(limits= c("", "0-1", "1-2","2-3", "3-4", "4-5", "5-10", "10-15", "15-20", "20-25", "25-30", "30 +"))
##Income count
mula_count <- answers %>% count(Q9, sort = TRUE)
mula_count
```

As a "value filter", we filter for observations / respondents with (1) income >= $100,000 and (2) 10+ yrs experience. Our aim was to extract the "perspective" of those that are most experienced and highest income earning (regardless of educational background) to see whether their perspective is consistent with or different from that of the general survey populace:

```{r}
##Filter data for (1) income >= $100,000, and 10+ yrs experience
answers <- answers %>% 
  mutate(value_filter = ifelse((answers$Q8 == "10-15" | answers$Q8 == "15-20" | answers$Q8 == "20-25" | answers$Q8 == "25-30" | answers$Q8 == "30 +") & (answers$Q9 == "100-125,000" | answers$Q9 == "125-150,000" | answers$Q9 == "150-200,000" | answers$Q9 == "200-250,000" | answers$Q9 == "250-300,000" | answers$Q9 == "300-400,000" | answers$Q9 == "400-500,000" | answers$Q9 == "500,000+"), "yes", "no"))

#Thus, extracting 728 observations from more than 23,000
filtered <-answers[ which(answers$value_filter =="yes"),]

```

Later, we can apply this value filter to our dataset to see whether there are any observable differences between the general survey populace and those that have 10+ years experience and are earning $100k+ more per year. In short, *do those with more experience and market-value see things differently or similarly?*

>LEFT OFF HERE WITH WRITE UP<

At this point we could visualize and analyze data for software v general skills. Then we could visualize and analyze data with a "value filter" applied and see if there's any contrast or insight gained by pulling from a subset of the original data (ie. high income DSs).

Below is the code for our creation of a table for unique variables related to SOFTWARE SKILLS from the **2018 Kaggle ML & DS Survey** data **with our value filter applied**: 

```{r}
#Filtered "regular basis" software (Q16) count
##Gather associated answers into one column, cut out irrelevant columns, then count
#Note: observation count = 16 cols x 748 observations (in filtered)
f_reg <- gather(filtered, key = "Q16_A_ID", value = "Q16_Answer", Q16_Part_1, Q16_Part_2, Q16_Part_3, Q16_Part_4, Q16_Part_5, Q16_Part_6, Q16_Part_7, Q16_Part_8, Q16_Part_9, Q16_Part_10, Q16_Part_11, Q16_Part_12, Q16_Part_13, Q16_Part_14, Q16_Part_15, Q16_Part_16)
f_reg <- f_reg[ -c(1:20) ]
f_reg <- f_reg %>% count(Q16_Answer, sort = TRUE)%>% rename(f_reg = n)
f_reg <- f_reg [-1,] #remove empty row

#Filtered "use most often" software (Q17) count
f_mo <- filtered %>% count(Q17, sort = TRUE)%>% rename(f_mo = n)
f_mo <- f_mo[-2,] #remove empty row

#Software language "recommend to aspiring DSs" (Q18) count
f_rec <- filtered %>% count(Q18, sort = TRUE)%>% rename(f_rec = n)
f_rec <- f_rec[-3,] #remove empty row

#Plot visualization with "value filter" side-by-side with general data output for software and general skills
```
Discuss insight gained from value filter: Python is still #1 just the proportions changed and with a subset of the group (for Q16), SQL rather than R was #2.


Tidy and transform: put Jeff Hale’s in a form we can compare / pull from. *And then* introduce Jeff Hale’s pretty data for comparison and see what we can lean on.

Gather counts for unique variables related to SOFTWARE SKILLS for Jeff Hale's dataset:
```{r}
#Explore results for each job board
#Isolate and format relevant data for tech skills
tech_skills[ , c(2,3,4,5)]  <- apply(tech_skills[ , c(2,3,4,5)], 2, function(x) as.numeric(str_remove(x, ",")))
tech_skills_final <- tech_skills%>%
  filter(Keyword != "Total", LinkedIn.. != "")%>%
  select(Keyword:Monster)%>%
  mutate(total = rowSums(. [2:5]))%>%
  arrange(desc(total))
##Then sum all rows to get all job search results per keyword and re-plot
```

Gather counts for unique variables related to GENERAL SKILLS for Jeff Hale's dataset:
```{r}
#Explore results for each job board
#Isolate and format relevant data for general skills
genl_skills[ , c(2,3,4,5)]  <- apply(genl_skills[ , c(2,3,4,5)], 2, function(x) as.numeric(str_remove(x, ",")))
genl_skills_final <- genl_skills%>%
  select(Keyword:Monster)%>%
  filter(Keyword != "Total", LinkedIn != "")%>%
  mutate(total = rowSums(. [2:5]))%>%
  arrange(desc(total))

#Subset data for most popular observations: visualization, statistics, mathematics, machine learning, deep learning, computer science, communication, analysis
genl_skills_final <- genl_skills_final %>% filter(total > 3000)
```

--------------------------------------------------------------------------------

### VISUALIZE & ANALYZE

Plot SOFTWARE SKILLS survey results with and without the value filter, side-by-side.

```{r}
#Plot value filtered v general results side by side (ie. reg v. f_reg) as proportions
##Can we / Should we apply what Mustafa did for tech_skills here?
##The only real insight is a difference in proportions and SQL as #2 for Q16
```

Join the data frames to examine the job board patterns adjacent to the data scientist survey results.
```{r}
keyword_comp <- tech_skills_final %>%
  inner_join(rec,c("Keyword" = "Q18"))%>%
  left_join(reg,c("Keyword" = "Q16_Answer"))%>%
  left_join(mo,c("Keyword" = "Q17"))%>%
  pivot_longer(cols = LinkedIn:mo, names_to = "source" , values_to = "frequency" )%>% 
      group_by(source) %>%
      mutate(pct  = frequency / sum(frequency))
```


```{r}
keyword_comp%>%
  filter(source == "total" | source == "rec")%>%
  ggplot() +
  geom_bar(aes(x = Keyword , y = pct, fill = source), stat = "identity", position = "dodge", width = 1)+
  scale_fill_manual(labels = c("Data Scientist", "Job Board"), values = c("#9999CC", "#66CC99")) +
  labs( title = "Date Scientist Recomendations VS Job Board", x = "Software", y = "Share of Mentions", fill = "Source")+
  coord_flip()
```

Python is the clear favorite, but let's remove it to get a better landscape of the remaining software.
```{r}
keyword_comp%>%
  filter(source == "total" | source == "rec", Keyword != "Python")%>%
  ggplot() +
  geom_bar(aes(x = Keyword , y = pct, fill = source), stat = "identity", position = "dodge", width = 1)+
  scale_fill_manual(labels = c("Data Scientist", "Job Board"), values = c("#9999CC", "#66CC99")) +
  labs( title = "Date Scientist Recomendations VS Job Board", x = "Software", y = "Share of Mentions", fill = "Source")+
  coord_flip()
```


Python > R > SQL. **Top skills being Machine Learning, Analysis, Statistics.**
**Do the descrepancies in mentions relate to certain deficiencies in the field, such as Scala, SAS, and Java**


Plot for value filter (v. regular survey data)

Examine job board patterns for general skills.

```{r}
#Plot for general skills from Jeff Hale's dataset

ggplot(genl_skills_final) +
  geom_bar(aes(x = Keyword , y = total, fill = Keyword), stat = "identity", position =     "dodge", width = 1) + 
  coord_flip()+ 
  theme(legend.position = "none") +
  labs( title = "Job Board General Skills Mentions", x = "", y = "", fill = "Source")
```

Visualization, Statistics, Mathematics, Machine Learning, Computer Science, Communication, Analysis*. **Top skills being Machine Learning, Analysis, Statistics.**

--------------------------------------------------------------------------------

### CONCLUDE

Interpret the visualizations and analysis to draw a clear, concise response to 
> What are the most valued data science skills?

Discussion of General Survey Results --> Value Filter --> Jeff Hale's Set

Discussion of how our approach changed.

Discussion of major insights gained. What is the answer to the Q?