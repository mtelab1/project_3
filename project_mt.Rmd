---
title: "DATA 607 - Project 3"
author: "Team Silver Fox"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    code_folding: hide
    df_print: paged
    highlight: tango
    #number_sections: yes
    smooth_scroll: yes
    theme: united
    toc_collapsed: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    extra_dependencies:
      -geometry
      -multicol
      -multirow
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '5'
    toc_float: yes
  word_document:
    toc: year
    toc_depth: "5"
theme: lumen
number_sections: yes
toc_depth: 3
---


```{r load-packages, include=FALSE}
library(tidyverse)
library(openintro)
library(dplyr)
library(tidyr)
library(RCurl)
library(ggplot2)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### BACKGROUND

> What are the most valued data science skills?

Answering this question formed the crux of our project. 

We collaborated in understanding the question at hand, brainstorming an approach and where we might pull data from, deciding on a dataset and then acquiring, tidying & transforming, visualizing & analyzing this dataset before ultimately coming to the conclusion we'll present later.

*Team Silver Fox*  collaborated virtually on Slack, Google Docs, Google Meet, and Github. There were numerous adaptations to our approach along the way and we ultimately decided on a "twice sifted" approach to answering the question above.

Without further adieu, let’s introduce *Team Silver Fox*:

* **Dan Rosenfeld** – [Github link]
* **Magnus Skonberg** – [Github](https://github.com/Magnus-PS/)
* **Mustafa Telab** – [Github](https://github.com/mtelab1)
* **Josef Waples** - [Github](https://github.com/hachiko1024)

--------------------------------------------------------------------------------

#### DATA SOURCE(S)

We identified our sources of data as the following (cited APA-style below):

1. Kaggle. (2018). **2018 Kaggle ML & DS Survey** [Data file]. Retrieved from https://www.kaggle.com/kaggle/kaggle-survey-2018?select=multipleChoiceResponses.csv

2. Jeff Hale. (2018). **The Most in Demand Skills for Data Scientists** [Data file]. Retrieved from https://www.kaggle.com/discdiver/the-most-in-demand-skills-for-data-scientists/log?select=ds_job_listing_software.csv

For more background on Data Source #1:

* The **2018 Kaggle ML & DS Survey** dataset had (3) .csv’s: the Survey Schema, Free Form Responses, and Multiple Choice Responses. 

* *The Survey Schema* contained questions and explanations of response exemptions. While we do not plan on analyzing this .csv, it was useful pre-screening questions for pertinence. 

* *The Free Form Responses* contained text answers submitted by survey takers when their response was not covered by multiple choice options. If we deem this set valuable / unique for purposes of analysis, we will draw on it. Otherwise, it may be disregarded as well.

* *The Multiple Choice Responses* contain categorical variables submitted by respondents. This is where our “gold” lies and likely where the brunt of our data tidying, transforming, and analysis efforts will go.

--------------------------------------------------------------------------------

#### APPROACH

We started our search by utilizing Kaggle’s self proclaimed “most comprehensive dataset available on the state of ML and data science”, pulling what we deemed useful from this set.

From our findings of the general dataset, we then applied a “value filter” (ie. only considering responses above a certain income, education, or experience level) to see how the general perspective of essential skills differed from that of a more seasoned (or market-valued) perspective.

At this point, we then transitioned to comparing data science skills deemed as valuable in Kaggle's 2018 survey with the most in-demand skills for data scientists across multiple job sites. This "pro level" dataset (also from 2018) was prepared by Jeff Hale, and drawn on to paint the picture of contrast (if there was any) between the skills data scientists deem as most valuable (ie. those doing the work) vs. those that employers hire for (ie. hiring managers).

By contrasting our sets via different techniques and visualizations, our findings were twice sifted. Hence the mention of a "twice sifted" approach in the *Background* section.

Our general approach is outlined below:

1. **Pre-screen:** after viewing the initial Kaggle **2018 Kaggle ML & DS Survey** multiple choice response dataset and seeing that we would not be able to access this dataset via Github due to its size (39+ MB), we deemed “pre-screening” essential. We reviewed all 50 questions from the SurveySchema.csv file and kept only those applicable to the question at hand. Once this dataset had been “pre-screened” it was uploaded to Github along with Jeff Hale’s general and software skill csv files (for later comparison).

2. **Acquire data via reading .csv's:** get the URL, read each .csv from Github (in its raw form), and put the data into tabular form before moving on to tidying and transforming.
3. **Tidy and transform:** by leaning on the dplyr() library and numerous useful R functions, we extracted a table of questions and a table of answers prior to extracting relevant keywords and counts (for later visualization). Our T&T process was heavy on the **2018 Kaggle ML & DS Survey** data and light for the **The Most in Demand Skills for Data Scientists** data. 
4. **Visualize and analyze**: once our data was in a more "digestible" form we set out to generate insightful plots regarding the most valuable **software skills** and **general skills**. We plotted (and interpreted) the survey data, and then applied "value filter" to this data prior to introducing the plots for the **The Most in Demand Skills for Data Scientists** data. The purpose of this tiered approach was to sift for additional insight (if there was any to be found).
5. **Conclude**: after applying our tiered approach, we drew insight from the story our data told and came to a conclusion. More on this later :)

--------------------------------------------------------------------------------

### ACQUIRE DATA

First, we read our .csv's from Github (in their raw form) and then converted these datasets into tabular form. Our approach was to create and utilize a function to get corresponding URLs and read the csv files from these URLs. This was not the most line-efficient strategy but it was a unique way of extracting our csv's which some may deem a "rinse and repeat" process:
```{r}
#Store URLs
url1 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/pertinent_mc_resps.csv"
url2 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/ds_software.csv"
url3 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/ds_general.csv"

#Function to get URLs and read in corresponding .csv files
url_to_dataframe_function <- function(x, y, z) {
  url <- getURL(url1)
  data1 <- read.csv(text = url)
  
  url2 <- getURL(url2)
  data2 <- read.csv(text = url2)
  
  url3 <- getURL(url3)
  data3 <- read.csv(text = url3)
  
  return(list(data1, data2, data3))
  }

#Call to function (defined above)
dataframes <- url_to_dataframe_function(url1, url2, url3)

#Assignment of values
for (i in seq(dataframes))
  assign(paste0("data", i), dataframes[[i]])

#Conversion to dataframe and display for 2018 Kaggle ML & DS Survey multiple choice data and then The Most in Demand Skills for Data Scientists software and general skills
mc_resps <- as_tibble(data1)
tech_skills <- as_tibble(data2)
genl_skills <- as_tibble(data3)

```

--------------------------------------------------------------------------------

### TIDY & TRANSFORM

Once we had our data at hand, we set out to make sense of this data and put it in a more digestible (normalized) form for later visualization and analysis.

For this phase of the project, we added to the more ordinary of R functions with our use of the dplyr() library. We removed columns and rows, renamed columns, transposed our data into long form, gathered essential columns into different data frames, extracted relevant counts, and created "light" visualizations meant to provide insight as to what v. wasn't valuable and essential for the next phase. 

Our T&T process was heavy on the **2018 Kaggle ML & DS Survey** data and light for the **The Most in Demand Skills for Data Scientists** data, and took the following form: 

We created a table of questions:
```{r}
#Extract all unique questions into questions table
questions <- mc_resps[1,]
colnames(questions)
#Compress questions with multiple columns into one
comp_qs <- questions[ -c(6:12,14:31,33,35,38:43) ]
#Rename column titles for same format (ie. Q4)
names(comp_qs)[names(comp_qs) == "Q11_Part_1"] <- "Q11"
names(comp_qs)[names(comp_qs) == "Q16_Part_1"] <- "Q16"
names(comp_qs)[names(comp_qs) == "Q34_Part_1"] <- "Q34"
#Transpose for a long dataframe
qs_long <- gather(comp_qs, "Q #", "Question", 1:10, factor_key=TRUE)
#Replace the question number with a question ID (ie. Q4 --> 1)
qs_long$Q_ID <- seq.int(nrow(qs_long)) 
qs_long <- qs_long[ -c(1) ]
qs_final <- qs_long[c("Q_ID", "Question")]
qs_final
```

We created a table of Answers:
```{r}
#Further widdle our dataset down: remove all columns with "OTHER" labels and non-valuable entries, remove 1st row (question statement)
answers <- mc_resps[ -c(11:12,29:31,33,35,43) ]
answers <- answers[-1,]
```

Then we set out to draw counts from this table of Answers for Software Skills, General Skills, and the Value Filter ...

Below is the code for our creation of a table for unique variables related to SOFTWARE SKILLS from the **2018 Kaggle ML & DS Survey** data: 
```{r}
#Software language "regular basis" (Q16) count
##Gather associated answers into one column, cut out irrelevant columns, then count
reg <- gather(answers, key = "Q16_A_ID", value = "Q16_Answer", Q16_Part_1, Q16_Part_2, Q16_Part_3, Q16_Part_4, Q16_Part_5, Q16_Part_6, Q16_Part_7, Q16_Part_8, Q16_Part_9, Q16_Part_10, Q16_Part_11, Q16_Part_12, Q16_Part_13, Q16_Part_14, Q16_Part_15, Q16_Part_16)
reg <- reg[ -c(1:19) ]
reg <- reg %>% count(Q16_Answer, sort = TRUE)%>% rename(reg = n)
reg <- reg [-1,] #remove empty row
reg
#Software language "use most often" (Q17) count
mo <- answers %>% count(Q17, sort = TRUE)%>% rename(mo = n)
mo <- mo[-1,] #remove empty row
mo
#Software language "recommend to aspiring DSs" (Q18) count
rec <- answers %>% count(Q18, sort = TRUE)%>% rename(rec = n)
rec <- rec[-2,] #remove empty row
rec
```

Below is the code for our creation of a table for unique variables related to GENERAL SKILLS from the **2018 Kaggle ML & DS Survey** data: 

>can we use this data for comparison plots (in V&A section)?<

```{r}
###Tidying: could pull keywords out for better plotting, analysis, etc. Phrases are too long to plot
#Machine learning (Q10) count
answers %>% count(Q10, sort = TRUE)
#Daily activities (Q11) count
##Gather associated answers into one column, cut out irrelevant columns, then count
all_daily <- gather(answers, key = "Q11_A_ID", value = "Q11_Answer", Q11_Part_1, Q11_Part_2, Q11_Part_3, Q11_Part_4, Q11_Part_5, Q11_Part_6)
all_daily <- all_daily[ -c(1:29) ]
all_daily %>% count(Q11_Answer, sort = TRUE)
#Software activity (Q23) count
sw <- answers %>% count(Q23, sort = TRUE)
sw <- sw[-2,] #remove empty row
sw$Q23 <- as.character(sw$Q23)
sw$Q23[sw$Q23 == "50% to 74% of my time"] <- "50%-74%"
sw$Q23[sw$Q23 == "25% to 49% of my time"] <- "25%-49%"
sw$Q23[sw$Q23 == "1% to 25% of my time"] <- "1%-25%"
sw$Q23[sw$Q23 == "75% to 99% of my time"] <- "75%-99%"
sw$Q23[sw$Q23 == "0% of my time"] <- "0%"
sw$Q23[sw$Q23 == "100% of my time"] <- "100%"
sw
#Visualize software activity
ggplot(sw, aes(x = Q23, y = n)) +
  geom_col() +
  labs(title = "Data Science Survey", subtitle = "% of school / workday actively coding", x = "", y = "") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5)) +
  labs(caption = "2018") +
  scale_x_discrete(limits= c("0%", "1%-25%", "25%-49%","50%-74%", "75%-99%", "100%"))
#% use time (Q34) count
##Gather associated answers into one column, cut out irrelevant columns, then count
all_time <- gather(answers, key = "Q34_A_ID", value = "Q34_Answer", Q34_Part_1, Q34_Part_2, Q34_Part_3, Q34_Part_4, Q34_Part_5, Q34_Part_6)
all_time <- all_time[ -c(1:29) ]
all_time %>% count(Q34_Answer, sort = TRUE)
```

Below is the code for our creation of a table for unique variables related to the VALUE FILTER from the **2018 Kaggle ML & DS Survey** data: 

>These can then be plotted, graphed, etc. or used to build a "filter" by considering only entries where, for instance, the respondent makes >=$100k ...<

```{r}
#Education level count
answers %>% count(Q4, sort = TRUE)
##Experience count
answers$Q8 <- as.character(answers$Q8)
answers$Q8[answers$Q8 == "10-May"] <- "5-10"
answers$Q8[answers$Q8 == "15-Oct"] <- "10-15"
answers$Q8[answers$Q8 == "4-Mar"] <- "3-4"
answers$Q8[answers$Q8 == "2-Jan"] <- "1-2"
answers$Q8[answers$Q8 == "3-Feb"] <- "2-3"
answers$Q8[answers$Q8 == "5-Apr"] <- "4-5"
#Storage of corresponding observation counts in "a"
exp_count <- answers %>% count(Q8, sort = TRUE)
#Visualization of years experience count
ggplot(exp_count, aes(x = Q8, y = n)) +
  geom_col() +
  labs(title = "Data Science Survey", subtitle = "Years of Experience") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5)) +
  labs(caption = "2018", x = "", y = "") +
  scale_x_discrete(limits= c("", "0-1", "1-2","2-3", "3-4", "4-5", "5-10", "10-15", "15-20", "20-25", "25-30", "30 +"))
##Income count
mula_count <- answers %>% count(Q9, sort = TRUE)
mula_count
```

>LEFT OFF HERE WITH WRITE UP<

At this point we could visualize and analyze data for software v general skills. Then we could visualize and analyze data with a "value filter" applied and see if there's any contrast or insight gained by pulling from a subset of the original data (ie. high income DSs).


```{r}
#Apply a "value filter" to our Data Science Survey data
#Filter observations for 10+ yrs experience and $100k+ income
#Extract data where column entries meet our criteria for Q8 and Q9:
#value_filter1 <- answers$Q9 == "100-125,000" 
#value_filter1
#ten_plus <- answers$Q9
#hundredk_plus <-
  
#Plot visualization with "value filter" side-by-side with general data output for software and general skills
```

Tidy and transform: put Jeff Hale’s in a form we can compare / pull from. *And then* introduce Jeff Hale’s pretty data for comparison and see what we can lean on.

Gather counts for unique variables related to SOFTWARE SKILLS for Jeff Hale's dataset:
```{r}
#Explore results for each job board
#Isolate and format relevant data for tech skills
tech_skills[ , c(2,3,4,5)]  <- apply(tech_skills[ , c(2,3,4,5)], 2, function(x) as.numeric(str_remove(x, ",")))
tech_skills_final <- tech_skills%>%
  filter(Keyword != "Total", LinkedIn.. != "")%>%
  select(Keyword:Monster)%>%
  mutate(total = rowSums(. [2:5]))%>%
  arrange(desc(total))
##Then sum all rows to get all job search results per keyword and re-plot
```

Gather counts for unique variables related to GENERAL SKILLS for Jeff Hale's dataset:
```{r}
#Explore results for each job board
#Isolate and format relevant data for general skills
genl_skills[ , c(2,3,4,5)]  <- apply(genl_skills[ , c(2,3,4,5)], 2, function(x) as.numeric(str_remove(x, ",")))
genl_skills_final <- genl_skills%>%
  select(Keyword:Monster)%>%
  filter(Keyword != "Total", LinkedIn != "")%>%
  mutate(total = rowSums(. [2:5]))%>%
  arrange(desc(total))
##Then sum all rows to get all job search results per keyword and re-plot
```

--------------------------------------------------------------------------------

### VISUALIZE & ANALYZE

Join the data frames to examine the job board patterns adjacent to the data scientist survey results.
```{r}
keyword_comp <- tech_skills_final %>%
  inner_join(rec,c("Keyword" = "Q18"))%>%
  left_join(reg,c("Keyword" = "Q16_Answer"))%>%
  left_join(mo,c("Keyword" = "Q17"))%>%
  pivot_longer(cols = LinkedIn:mo, names_to = "source" , values_to = "frequency" )%>% 
      group_by(source) %>%
      mutate(pct  = frequency / sum(frequency))
```


```{r}
keyword_comp%>%
  filter(source == "total" | source == "rec")%>%
  ggplot() +
  geom_bar(aes(x = Keyword , y = pct, fill = source), stat = "identity", position = "dodge", width = 1)+
  scale_fill_manual(labels = c("Data Scientist", "Job Board"), values = c("blue", "green")) +
  labs( title = "Date Scientist Recomendations VS Job Board", x = "Software", y = "Share of Mentions", fill = "Source")
```

Plot for value filter (v. regular survey data)

Plot for general skills.

--------------------------------------------------------------------------------

### CONCLUDE

Interpret the visualizations and analysis to draw a clear, concise response to 
> What are the most valued data science skills?