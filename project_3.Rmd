---
title: "DATA 607 - Project 3"
author: "Team Silver Fox"
date: "`r Sys.Date()`"
output: html_document
---


```{r load-packages, include=FALSE}
library(tidyverse)
library(openintro)
library(dplyr)
library(tidyr)
library(RCurl)
library(ggplot2)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### BACKGROUND

> What are the most valued data science skills?

Answering this question formed the crux of our project. 

We collaborated in understanding the question at hand, brainstorming an approach and where we might pull data from, deciding on a dataset and then acquiring, tidying & transforming, visualizing & analyzing this dataset before ultimately coming to the conclusion we'll present later.

Along the way, *Team Silver Fox*  collaborated virtually on Slack, Google Docs, Google Meet, and Github and [insert what we did – in short] in answering the question posed above.

Without further adieu, let’s introduce *Team Silver Fox*:

* **Dan Rosenfeld** – one sentence bio. [Github link]
* **Magnus Skonberg** – one sentence bio. [Github link]
* **Mustafa Telab** – one sentence bio. [Github link]
* **Josef Waples** – one sentence bio. [Github link].

### APPROACH EXPLANATION

We started our search by utilizing Kaggle’s self proclaimed “most comprehensive dataset available on the state of ML and data science”, pulling what we deemed useful from this set.

From our findings of the general dataset, we then applied a “value filter” (ie. only considering responses above a certain income, education, or experience level) to see how the general perspective of essential skills differed from that of a more seasoned (or market-valued) perspective.

At this point, we then transitioned to comparing data science skills deemed as valuable in Kaggle's 2018 survey with the most in-demand skills for data scientists across multiple job sites. This "pro level" dataset, prepared by Jeff Hale, was pulled in to paint the picture of contrast (if there was any) between the skills data scientists deem as most valuable (ie. those doing the work) vs. those that employers hire for (ie. hiring managers).

By contrasting our sets via different techniques and visualizations, we further refined our own findings and actually added a layer.

Our general approach is outlined below:

1. Pre-screen: after viewing the initial Kaggle (multipleChoiceResponses.csv) dataset and seeing that we would not be able to access this dataset via Github due to its size (39+ MB), we deemed “pre-screening” the dataset an essential step. We reviewed all 50 questions from the SurveySchema.csv file and marked those that we deemed applicable to providing insight for what the most valuable data science skills are. We then removed all inapplicable columns from the multipleChoiceResponses.csv. Once this dataset had been “pre-screened” it was uploaded to Github along with Jeff Hale’s general and software skill csv files (for later comparison).

<!--If this step is unclear please let me (Magnus) know. I can flesh out more exactly what the criteria was for keeping v. deleting columns (questions) ...-->

2. Acquire data: get the URL, read the .csv from Github (in its raw form), and put it in tabular form.
3. Tidy and transform: our Kaggle dataset and put Jeff Hale’s in a form we can compare / pull from.
4. Filter our dataset for another interpretation of “valuable” (education, income, experience level).
5. Interpret our filtered v our filtered datasets and see what we can pull.
6. Introduce Jeff Hale’s pretty data for comparison and see what we can lean on. Introduce general skills (something missing from our interpretation upto this point).
7. Visualize and analyze.
8. Conclude.

<!--This is a work-in-progress. There are probably a number of points above that could be better worded / fleshed out etc. I was just chug-a-lugging along today and wanted to jot out where our analysis could go based on the words "most valuable" ...-->

### ACQUIRE DATA

First we read our multiple choice dataset from the 2018 Kaggle survey.

```{r}
###Commented out for now###

#Store URLs
# url1 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/pertinent_mc_resps.csv"
# url2 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/ds_software.csv"
# url3 <- "https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/ds_general.csv"
# 
# #Function to get URLs and read in corresponding .csv files
# url_to_dataframe_function <- function(x, y, z) {
#   url <- getURL(url1)
#   data1 <- read.csv(text = url)
#   
#   url2 <- getURL(url2)
#   data2 <- read.csv(text = url2)
#   
#   url3 <- getURL(url3)
#   data3 <- read.csv(text = url3)
#   
#   return(list(data1, data2, data3))
# }
# 
# #Call to function (defined above)
# dataframes <- url_to_dataframe_function(url1, url2, url3)
# 
# #Assignment of values 
# for (i in seq(dataframes))
#   assign(paste0("data", i), dataframes[[i]])
# 
# #Conversion to dataframe and display
# mc_resps <- as_tibble(data1)
# tech_skills <- as_tibble(data2)
# genl_skills <- as_tibble(data3)

```

```{r}
#Get URL, read .csv (in raw form) from github, and put into tabular form
url <- getURL("https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/pertinent_mc_resps.csv")
data1 <- read.csv(text = url)
mc_resps <- as_tibble(data1)

#Show what we're working with:
#mc_resps

#Display dimensions: 23860 rows x 43 columns
#dim(mc_resps)

```

Next we download our technical skills dataset (for later comparison).

```{r}
#Get URL, read .csv (in raw form) from github, and put into tabular form
url2 <- getURL("https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/ds_software.csv")
data2 <- read.csv(text = url2)
tech_skills <- as_tibble(data2)

#Show what we're working with:
#tech_skills

#Our output is 44 rows x 12 columns
#dim(tech_skills)

```

Finally, we download our general skills dataset (for later comparison).

```{r}
#Get URL, read .csv (in raw form) from github, and put into tabular form
url3 <- getURL("https://raw.githubusercontent.com/Magnus-PS/CUNY-SPS-DATA-607/Project-3/ds_general.csv")
data3 <- read.csv(text = url3)
genl_skills <- as_tibble(data3)

#Show what we're working with:
#genl_skills

#Our output is 30 rows x 5 columns
#dim(genl_skills)

```

### TIDY & TRANSFORM

Create table of questions with Q_ID and Question as columns:

```{r}
#Extract all unique questions into questions table
questions <- mc_resps[1,]
colnames(questions)

#Compress questions with multiple columns into one
comp_qs <- questions[ -c(6:12,14:31,33,35,38:43) ]

#Rename column titles for same format (ie. Q4)
names(comp_qs)[names(comp_qs) == "Q11_Part_1"] <- "Q11"
names(comp_qs)[names(comp_qs) == "Q16_Part_1"] <- "Q16"
names(comp_qs)[names(comp_qs) == "Q34_Part_1"] <- "Q34"

#Transpose for a long dataframe
qs_long <- gather(comp_qs, "Q #", "Question", 1:10, factor_key=TRUE)

#Replace the question number with a question ID (ie. Q4 --> 1)
qs_long$Q_ID <- seq.int(nrow(qs_long)) 
qs_long <- qs_long[ -c(1) ]
qs_final <- qs_long[c("Q_ID", "Question")]
qs_final

```

Create the table of Answers (with Q_ID, A_ID, Answer as columns) *and then* gather counts for unique variables related to the Software Skills, General Skills, and the Value Filter:

```{r}
#Further widdle our dataset down: remove all columns with "OTHER" labels and non-valuable entries, remove 1st row (question statement)
answers <- mc_resps[ -c(11:12,29:31,33,35,43) ]
answers <- answers[-1,]

```

Gather counts for unique variables related to SOFTWARE SKILLS: 

```{r}
#Software language "regular basis" (Q16) count
##Gather associated answers into one column, cut out irrelevant columns, then count
reg <- gather(answers, key = "Q16_A_ID", value = "Q16_Answer", Q16_Part_1, Q16_Part_2, Q16_Part_3, Q16_Part_4, Q16_Part_5, Q16_Part_6, Q16_Part_7, Q16_Part_8, Q16_Part_9, Q16_Part_10, Q16_Part_11, Q16_Part_12, Q16_Part_13, Q16_Part_14, Q16_Part_15, Q16_Part_16)
reg <- reg[ -c(1:19) ]

reg <- reg %>% count(Q16_Answer, sort = TRUE)%>% rename(reg = n)
reg <- reg [-1,] #remove empty row
reg

#Software language "use most often" (Q17) count
mo <- answers %>% count(Q17, sort = TRUE)%>% rename(mo = n)
mo <- mo[-1,] #remove empty row
mo

#Software language "recommend to aspiring DSs" (Q18) count
rec <- answers %>% count(Q18, sort = TRUE)%>% rename(rec = n)
rec <- rec[-2,] #remove empty row
rec

```

Gather counts for unique variables related to GENERAL SKILLS: We may end up using this data in a different way than originally intended but that's TBD ...

```{r}
###Tidying: could pull keywords out for better plotting, analysis, etc. Phrases are too long to plot

#Machine learning (Q10) count
answers %>% count(Q10, sort = TRUE)

#Daily activities (Q11) count
##Gather associated answers into one column, cut out irrelevant columns, then count
all_daily <- gather(answers, key = "Q11_A_ID", value = "Q11_Answer", Q11_Part_1, Q11_Part_2, Q11_Part_3, Q11_Part_4, Q11_Part_5, Q11_Part_6)
all_daily <- all_daily[ -c(1:29) ]
all_daily %>% count(Q11_Answer, sort = TRUE)

#Software activity (Q23) count
sw <- answers %>% count(Q23, sort = TRUE)
sw <- sw[-2,] #remove empty row
sw$Q23 <- as.character(sw$Q23)
sw$Q23[sw$Q23 == "50% to 74% of my time"] <- "50%-74%"
sw$Q23[sw$Q23 == "25% to 49% of my time"] <- "25%-49%"
sw$Q23[sw$Q23 == "1% to 25% of my time"] <- "1%-25%"
sw$Q23[sw$Q23 == "75% to 99% of my time"] <- "75%-99%"
sw$Q23[sw$Q23 == "0% of my time"] <- "0%"
sw$Q23[sw$Q23 == "100% of my time"] <- "100%"
sw

#Visualize software activity

ggplot(sw, aes(x = Q23, y = n)) +
  geom_col() +
  labs(title = "Data Science Survey", subtitle = "% of school / workday actively coding") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5)) +
  labs(caption = "2018") +
  scale_x_discrete(limits= c("0%", "1%-25%", "25%-49%","50%-74%", "75%-99%", "100%"))


#% use time (Q34) count
##Gather associated answers into one column, cut out irrelevant columns, then count
all_time <- gather(answers, key = "Q34_A_ID", value = "Q34_Answer", Q34_Part_1, Q34_Part_2, Q34_Part_3, Q34_Part_4, Q34_Part_5, Q34_Part_6)
all_time <- all_time[ -c(1:29) ]
all_time %>% count(Q34_Answer, sort = TRUE)
```

Gather counts for unique variables related to the VALUE FILTER: These can then be plotted, graphed, etc. or used to build a "filter" by considering only entries where, for instance, the respondent makes >=$100k ...

```{r}
#Education level count
answers %>% count(Q4, sort = TRUE)

##Experience count
answers$Q8 <- as.character(answers$Q8)
answers$Q8[answers$Q8 == "10-May"] <- "5-10"
answers$Q8[answers$Q8 == "15-Oct"] <- "10-15"
answers$Q8[answers$Q8 == "4-Mar"] <- "3-4"
answers$Q8[answers$Q8 == "2-Jan"] <- "1-2"
answers$Q8[answers$Q8 == "3-Feb"] <- "2-3"
answers$Q8[answers$Q8 == "5-Apr"] <- "4-5"

#Storage of corresponding observation counts in "a"
exp_count <- answers %>% count(Q8, sort = TRUE)

#Visualization of years experience count
ggplot(exp_count, aes(x = Q8, y = n)) +
  geom_col() +
  labs(title = "Data Science Survey", subtitle = "Years of Experience") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5)) +
  labs(caption = "2018") +
  scale_x_discrete(limits= c("", "0-1", "1-2","2-3", "3-4", "4-5", "5-10", "10-15", "15-20", "20-25", "25-30", "30 +"))

##Income count
mula_count <- answers %>% count(Q9, sort = TRUE)
mula_count

```

At this point we could visualize and analyze data for software v general skills. Then we could visualize and analyze data with a "value filter" applied and see if there's any contrast or insight gained by pulling from a subset of the original data (ie. high income DSs).


```{r}
#Apply a "value filter" to our Data Science Survey data
#Filter observations for 10+ yrs experience and $100k+ income

#Extract data where column entries meet our criteria for Q8 and Q9:
value_filter1 <- answers$Q9 == "100-125,000" 
value_filter1
#ten_plus <- answers$Q9
#hundredk_plus <-
  
#Plot visualization with "value filter" side-by-side with general data output for software and general skills

```

Tidy and transform: put Jeff Hale’s in a form we can compare / pull from. *And then* introduce Jeff Hale’s pretty data for comparison and see what we can lean on.

Gather counts for unique variables related to SOFTWARE SKILLS for Jeff Hale's dataset:
```{r}
#Explore results for each job board
#Isolate and format relevant data for tech skills
tech_skills[ , c(2,3,4,5)]  <- apply(tech_skills[ , c(2,3,4,5)], 2, function(x) as.numeric(str_remove(x, ",")))
tech_skills_final <- tech_skills%>%
  filter(Keyword != "Total", LinkedIn.. != "")%>%
  select(Keyword:Monster)%>%
  mutate(total = rowSums(. [2:5]))%>%
  arrange(desc(total))

##Then sum all rows to get all job search results per keyword and re-plot
```

Gather counts for unique variables related to GENERAL SKILLS for Jeff Hale's dataset:
```{r}
#Explore results for each job board
#Isolate and format relevant data for general skills
genl_skills[ , c(2,3,4,5)]  <- apply(genl_skills[ , c(2,3,4,5)], 2, function(x) as.numeric(str_remove(x, ",")))
genl_skills_final <- genl_skills%>%
  select(Keyword:Monster)%>%
  filter(Keyword != "Total", LinkedIn != "")%>%
  mutate(total = rowSums(. [2:5]))%>%
  arrange(desc(total))
##Then sum all rows to get all job search results per keyword and re-plot
```

### ANALYZE

Join the data frames to examine the job board patterns adjacent to the data scientist survey results.
```{r}
keyword_comp <- tech_skills_final %>%
  inner_join(rec,c("Keyword" = "Q18"))%>%
  left_join(reg,c("Keyword" = "Q16_Answer"))%>%
  left_join(mo,c("Keyword" = "Q17"))%>%
  pivot_longer(cols = LinkedIn:mo, names_to = "source" , values_to = "frequency" )%>% 
      group_by(source) %>%
      mutate(pct  = frequency / sum(frequency))



```



```{r}

  ggplot(keyword_comp) +
  geom_bar(aes(x = Keyword , y = pct, fill = source), stat = "identity", position = "dodge", width = 1)
```

